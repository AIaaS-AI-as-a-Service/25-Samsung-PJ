{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PHM 데이터셋 고장 분류\n",
    "\n",
    "CNN-LSTM 모델을 사용하여 장비 고장 유형을 분류하는 실습 자료입니다.\n",
    "\n",
    "## 목표\n",
    "- 시계열 센서 데이터 전처리 방법 학습\n",
    "- 슬라이딩 윈도우 기법 이해 및 구현\n",
    "- CNN-LSTM 하이브리드 모델 구축\n",
    "- 고장 분류 모델 학습 및 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 라이브러리 Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 라이브러리\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 전처리\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# 딥러닝\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# 시드 고정 (재현성)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. PHM 데이터 로드\n\n**NASA Turbofan Engine Degradation (C-MAPSS) 데이터셋 사용**\n\n이 데이터셋은 항공기 터보팬 엔진의 Run-to-Failure 시뮬레이션 데이터로, 실제 PHM 연구에서 널리 사용됩니다.\n\n### 데이터 특성\n- **센서 개수**: 21개 (온도, 압력, 진동 등)\n- **운영 설정**: 3개 (고도, 속도, 스로틀)\n- **고장 유형**: \n  - 0: Normal (RUL > 100 cycles)\n  - 1: Early Degradation (50 < RUL ≤ 100)\n  - 2: Advanced Degradation (20 < RUL ≤ 50)\n  - 3: Critical (RUL ≤ 20)\n\n**Note**: 전처리된 데이터가 `../data/processed/` 폴더에 준비되어 있습니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# PHM 데이터 로드\n# Hint: pandas의 read_csv를 사용하여 '../data/processed/train_FD001_processed.csv' 파일을 로드하세요\n\nprint(\"NASA Turbofan Engine Degradation 데이터 로드 중...\")\n\n# 여기에 코드를 작성하세요\ndf = # TODO: CSV 파일 로드\n\nprint(f\"\\n데이터 로드 완료!\")\nprint(f\"데이터 shape: {df.shape}\")\n\n# 데이터 탐색\n# Hint: df.head(), df.info(), df.describe() 등을 활용하여 데이터를 살펴보세요\n# TODO: 엔진 개수 출력\n# TODO: 고장 유형 분포 출력"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 데이터 탐색 (EDA)\n",
    "\n",
    "**Hint**: 데이터의 분포, 고장 유형별 특성, 센서 간 상관관계 등을 시각화하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 데이터 탐색 (EDA)\n# Hint: 데이터의 기본 통계, 분포, 시각화를 통해 데이터를 이해하세요\n\n# 센서 컬럼 추출\nsensor_cols = [col for col in df.columns if col.startswith('sensor_')]\nprint(f\"센서 개수: {len(sensor_cols)}\")\n\n# TODO: df.describe()를 사용하여 센서 데이터의 기본 통계 확인\n\n# TODO: 고장 유형별 분포 시각화\n# Hint: matplotlib의 subplot을 사용하여 bar chart와 pie chart 2개를 그리세요\n# - plt.figure(figsize=(12, 5))\n# - plt.subplot(1, 2, 1) - bar chart\n# - plt.subplot(1, 2, 2) - pie chart\n# - df['fault_type'].value_counts()를 활용"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 데이터 전처리\n",
    "\n",
    "### 4.1 특징(X)과 타겟(y) 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 특징(X)과 타겟(y) 분리\n# Hint: 센서 컬럼만 선택하여 X로, fault_type 컬럼을 y로 분리하세요\n\nsensor_cols = [col for col in df.columns if col.startswith('sensor_')]\n\n# TODO: X와 y 분리\nX = # TODO: 센서 데이터만 선택하여 numpy array로 변환\ny = # TODO: fault_type 컬럼을 numpy array로 변환\n\nprint(f\"X shape: {X.shape}\")\nprint(f\"y shape: {y.shape}\")\nprint(f\"센서 개수: {len(sensor_cols)}\")\nprint(f\"고장 유형 개수: {len(np.unique(y))}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 데이터 정규화\n",
    "\n",
    "**Hint**: StandardScaler를 사용하여 각 센서 데이터를 정규화하세요. 이는 학습 속도와 성능을 향상시킵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 데이터 정규화\n# Hint: StandardScaler를 사용하여 센서 데이터를 정규화하세요\n# 정규화는 평균 0, 표준편차 1로 변환하여 학습 성능을 향상시킵니다\n\n# TODO: StandardScaler 생성 및 fit_transform\nscaler = # TODO: StandardScaler 객체 생성\nX_scaled = # TODO: X에 fit_transform 적용\n\nprint(f\"정규화 전 - 평균: {X.mean():.4f}, 표준편차: {X.std():.4f}\")\nprint(f\"정규화 후 - 평균: {X_scaled.mean():.4f}, 표준편차: {X_scaled.std():.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 슬라이딩 윈도우 생성\n",
    "\n",
    "시계열 데이터를 고정 길이의 윈도우로 분할합니다.\n",
    "\n",
    "**Hint**: \n",
    "- `window_size`: 한 샘플에 포함될 시간 스텝 수\n",
    "- `stride`: 윈도우를 이동시키는 간격\n",
    "- 각 윈도우의 마지막 시점의 레이블을 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_sliding_windows(X, y, window_size=50, stride=5):\n    \"\"\"\n    슬라이딩 윈도우를 사용하여 시계열 데이터를 변환\n    \n    Parameters:\n    - X: 입력 데이터 (n_samples, n_features)\n    - y: 레이블 (n_samples,)\n    - window_size: 윈도우 크기\n    - stride: 윈도우 이동 간격\n    \n    Returns:\n    - X_windows: (n_windows, window_size, n_features)\n    - y_windows: (n_windows,)\n    \"\"\"\n    \n    X_windows = []\n    y_windows = []\n    \n    # TODO: for 루프를 사용하여 stride 간격으로 윈도우를 생성하세요\n    # Hint:\n    # - range(0, len(X) - window_size + 1, stride)를 사용\n    # - 각 윈도우는 X[i:i+window_size] 형태로 슬라이싱\n    # - 레이블은 윈도우의 마지막 시점 y[i+window_size-1] 사용\n    # - X_windows.append(window), y_windows.append(label)\n    \n    \n    return np.array(X_windows), np.array(y_windows)\n\n# 윈도우 생성\nwindow_size = 50  # 50 사이클을 하나의 샘플로\nstride = 5        # 5 사이클씩 이동\n\nX_windows, y_windows = create_sliding_windows(X_scaled, y, window_size, stride)\n\nprint(f\"윈도우 설정: window_size={window_size}, stride={stride}\")\nprint(f\"원본 데이터: {X_scaled.shape} → 윈도우 데이터: {X_windows.shape}\")\nprint(f\"생성된 윈도우 개수: {len(X_windows):,}개\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 One-hot Encoding\n",
    "\n",
    "**Hint**: to_categorical을 사용하여 레이블을 one-hot 벡터로 변환하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# One-hot Encoding\n# Hint: keras.utils.to_categorical을 사용하여 정수 레이블을 one-hot 벡터로 변환하세요\n# 예: 2 → [0, 0, 1, 0]\n\n# TODO: y_windows를 one-hot 벡터로 변환\ny_categorical = # TODO: to_categorical 사용\n\nprint(f\"원본 레이블 shape: {y_windows.shape}\")\nprint(f\"One-hot 레이블 shape: {y_categorical.shape}\")\nprint(f\"\\n예시: {y_windows[0]} → {y_categorical[0]}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train/Test 데이터 분리\n# Hint: train_test_split을 사용하여 80:20 비율로 분리하세요\n# stratify 파라미터를 사용하여 클래스 비율을 유지하세요\n\n# TODO: train_test_split으로 데이터 분리\nX_train, X_test, y_train, y_test = # TODO: 코드 작성\n\nprint(f\"학습 데이터: {X_train.shape}\")\nprint(f\"테스트 데이터: {X_test.shape}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. CNN-LSTM 모델 구축\n",
    "\n",
    "### 모델 아키텍처\n",
    "1. **Conv1D**: 센서 간 공간적 특징 추출\n",
    "2. **LSTM**: 시간적 패턴 학습\n",
    "3. **Dense**: 분류\n",
    "\n",
    "**Hint**: \n",
    "- Input shape: (window_size, n_features)\n",
    "- Conv1D 레이어 후 MaxPooling 적용\n",
    "- LSTM 레이어는 return_sequences=False로 설정\n",
    "- 최종 Dense 레이어는 softmax 활성화 함수 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def build_cnn_lstm_model(input_shape, n_classes):\n    \"\"\"\n    CNN-LSTM 모델 구축\n    \n    아키텍처 힌트:\n    1. Conv1D → BatchNorm → MaxPooling → Dropout (2~3번 반복)\n       - filters: 64 → 128 → 256으로 증가\n       - kernel_size: 3\n       - padding: 'same'\n    2. LSTM → Dropout\n       - units: 128\n       - return_sequences: False\n    3. Dense → BatchNorm → Dropout\n       - units: 64\n    4. Output Dense (softmax)\n       - units: n_classes\n    \n    Parameters:\n    - input_shape: (window_size, n_features)\n    - n_classes: 분류할 클래스 개수\n    \"\"\"\n    \n    model = models.Sequential()\n    \n    # TODO: CNN 블록들을 추가하세요\n    # Hint: model.add(layers.Conv1D(...))\n    \n    \n    # TODO: LSTM 레이어를 추가하세요\n    # Hint: model.add(layers.LSTM(...))\n    \n    \n    # TODO: Dense 레이어들을 추가하세요\n    \n    \n    # TODO: 출력 레이어 추가\n    # Hint: model.add(layers.Dense(n_classes, activation='softmax'))\n    \n    \n    return model\n\n# 모델 생성 및 컴파일\ninput_shape = (window_size, X_scaled.shape[1])\nn_classes = y_categorical.shape[1]\n\nmodel = build_cnn_lstm_model(input_shape, n_classes)\n\n# TODO: 모델 컴파일\n# Hint: model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n\nmodel.summary()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 모델 학습\n",
    "\n",
    "**Hint**: \n",
    "- EarlyStopping: val_loss가 개선되지 않으면 조기 종료\n",
    "- ModelCheckpoint: 최고 성능 모델 저장\n",
    "- ReduceLROnPlateau: 학습률 동적 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 콜백 설정\n# Hint: EarlyStopping, ModelCheckpoint, ReduceLROnPlateau를 사용하여 학습 최적화\n\n# TODO: 콜백 리스트 생성\n# Hint:\n# - EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n# - ModelCheckpoint('../models/best_model.h5', monitor='val_accuracy', save_best_only=True)\n# - ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=7, min_lr=1e-7)\n\ncallbacks = # TODO: 콜백 리스트 작성\n\n# TODO: 모델 학습\n# Hint: model.fit(X_train, y_train, validation_split=0.2, epochs=100, batch_size=32, callbacks=callbacks)\n\nhistory = # TODO: 학습 코드 작성\n\nprint(\"\\n학습 완료!\")\nprint(f\"최종 학습 정확도: {history.history['accuracy'][-1]:.4f}\")\nprint(f\"최종 검증 정확도: {history.history['val_accuracy'][-1]:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 학습 결과 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 학습 곡선 시각화\n# Hint: history.history에서 'loss', 'val_loss', 'accuracy', 'val_accuracy'를 추출하여 시각화하세요\n\n# TODO: 2개의 subplot으로 Loss와 Accuracy 그래프 그리기\n# Hint:\n# - plt.figure(figsize=(14, 5))\n# - plt.subplot(1, 2, 1) - Loss 그래프\n# - plt.subplot(1, 2, 2) - Accuracy 그래프\n# - plt.plot(history.history['loss'], label='Train Loss')\n# - plt.plot(history.history['val_loss'], label='Val Loss')\n\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 모델 평가\n",
    "\n",
    "**Hint**: 테스트 데이터로 모델 성능을 평가하고, Confusion Matrix와 Classification Report를 출력하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 테스트 데이터로 예측 및 평가\n# Hint: model.predict()로 예측 후 np.argmax()로 클래스 추출\n\n# TODO: 예측 수행\ny_pred_prob = # TODO: model.predict(X_test)\ny_pred = # TODO: np.argmax로 클래스 추출\ny_true = # TODO: y_test에서 클래스 추출\n\n# TODO: 정확도 계산\n# Hint: sklearn.metrics.accuracy_score 사용\naccuracy = # TODO: 정확도 계산\nprint(f\"테스트 정확도: {accuracy:.4f}\")\n\n# TODO: Classification Report 출력\n# Hint: classification_report(y_true, y_pred, target_names=...)\n\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Confusion Matrix 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Confusion Matrix 시각화\n# Hint: confusion_matrix를 계산하고 seaborn의 heatmap으로 시각화하세요\n\n# TODO: Confusion Matrix 계산\ncm = # TODO: confusion_matrix(y_true, y_pred)\n\n# TODO: Heatmap으로 시각화\n# Hint:\n# - plt.figure(figsize=(10, 8))\n# - sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=..., yticklabels=...)\n# - plt.title, plt.xlabel, plt.ylabel 추가\n\n\n# 추가 과제: 정규화된 Confusion Matrix도 그려보세요\n# Hint: cm을 각 행의 합으로 나누어 백분율로 변환\n\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 9. 결과 분석 및 개선 방향\n\n**질문**:\n1. 모델의 성능이 만족스러운가요?\n2. 어떤 클래스가 가장 잘 분류되었나요?\n3. 어떤 클래스가 혼동되기 쉬운가요?\n4. 성능을 개선하기 위해 시도할 수 있는 방법은?\n\n**개선 방법**:\n\n### 1. 데이터 증강\n- **Stride 조정**: 현재 5 → 1~3으로 감소하여 더 많은 샘플 생성\n- **다른 데이터셋 추가**: FD002, FD003, FD004 데이터 활용\n- **시계열 증강**: Jittering, Scaling, Time Warping\n\n### 2. 하이퍼파라미터 튜닝\n- **Window size**: 30, 50, 100 등 다양한 크기 실험\n- **LSTM units**: 64, 128, 256\n- **Learning rate**: 0.0001 ~ 0.01\n- **Batch size**: 16, 32, 64\n\n### 3. 모델 아키텍처\n- **Bidirectional LSTM**: 양방향 시계열 패턴 학습\n- **Attention Mechanism**: 중요한 시점 강조\n- **Residual Connections**: 깊은 네트워크 학습\n\n### 4. 클래스 불균형 처리\n- **Class Weights**: 희소 클래스에 높은 가중치\n- **SMOTE**: 오버샘플링\n- **Focal Loss**: 어려운 샘플에 집중\n\n### 5. 실습 과제\n- 윈도우 크기를 변경해보고 성능 변화를 관찰하세요\n- LSTM units 수를 조정해보세요\n- 추가 Conv1D 레이어를 추가하거나 제거해보세요\n- FD002 데이터셋으로 학습해보세요 (더 복잡한 운영 조건)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 추가 실험 코드를 작성하세요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 모델 저장 및 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 최종 모델 및 전처리기 저장\n# Hint: model.save()로 모델 저장, pickle로 scaler와 설정 저장\n\n# TODO: 모델 저장\n# Hint: model.save('../models/final_cnn_lstm_model.h5')\n\n\n# TODO: Scaler 저장 (추후 실시간 예측에 필요)\n# Hint: pickle.dump(scaler, open('파일경로', 'wb'))\nimport pickle\n\n\n# TODO: 모델 설정 저장 (윈도우 크기, 센서 개수 등)\n# Hint: 딕셔너리로 설정을 만들고 pickle로 저장\nconfig = {\n    'window_size': window_size,\n    'stride': stride,\n    # TODO: 추가 설정 항목들\n}\n\n\n# 모델 로드 예시 (참고용)\n# loaded_model = keras.models.load_model('../models/final_cnn_lstm_model.h5')\n# with open('../models/scaler.pkl', 'rb') as f:\n#     loaded_scaler = pickle.load(f)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 완료!\n",
    "\n",
    "수고하셨습니다. 이 노트북을 통해 CNN-LSTM 모델을 사용한 고장 분류의 전체 파이프라인을 학습하셨습니다.\n",
    "\n",
    "추가 학습을 위해 solution 노트북을 참고하세요."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}